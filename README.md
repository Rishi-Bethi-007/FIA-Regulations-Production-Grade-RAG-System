ğŸ“˜ FIA Regulations Production-Grade RAG System

An industry-grade Retrieval-Augmented Generation (RAG) system built over FIA Formula 1 / Formula 2 / Formula 3 regulations (2018â€“2026), designed with scalability, latency, and evaluation in mind.

This project goes beyond a demo RAG:

uses Pinecone for scalable vector search

Redis caching for real performance gains

cross-encoder reranking (not LLM rerank)

metadata-aware retrieval (season, article, series, regulation type)

guardrails + evaluation harness (latency & faithfulness)

ğŸ” What problem does this solve?

Regulatory documents (like FIA rules) are:

long, dense, and frequently updated

spread across seasons, series, and revisions

difficult to search precisely (e.g. â€œArticle 12.3 in 2026 F1 Sporting Regulationsâ€)

This system allows users to:

ask natural language questions

retrieve exact regulation clauses

get grounded answers with citations

while maintaining low latency at scale

ğŸ§  High-Level Architecture
PDFs â†’ Chunking â†’ Embeddings â†’ Pinecone (Vector DB)
                           â†˜ SQLite DocStore
User Query
  â†’ Planner (filters, compare logic)
  â†’ Pinecone Retrieval (cached)
  â†’ Cross-Encoder Reranker
  â†’ Guardrails
  â†’ LLM Answer Generation
  â†’ Evaluation (latency + faithfulness)


Key design goals:

Fast (Redis caching, rerank limits)

Accurate (metadata filters + reranker)

Measurable (evaluation suite)

Production-ready (clear abstractions)

âœ¨ Key Features
ğŸ”¹ Retrieval & Search

Pinecone vector database (scales to 10k+ documents)

Metadata filtering:

season (2018â€“2026)

series (F1 / F2 / F3)

regulation type (sporting / technical / operational)

article references (e.g. 12.3)

Namespace strategy for safe re-indexing

ğŸ”¹ Performance

Redis caching:

embedding cache

retrieval cache

Measured cache hit rates

Cross-encoder reranking (ms-marco-MiniLM) for precision

ğŸ”¹ Safety & Quality

Input guardrails (prompt injection detection)

Context guardrails (tenant isolation, empty chunks)

Output guardrails (citation enforcement)

Faithfulness evaluation using an LLM judge

ğŸ”¹ Evaluation

Latency metrics: mean / p50 / p95

Cache hit-rate reporting

Faithfulness scoring against retrieved evidence

ğŸ“‚ Repository Structure
chunking/          â†’ sentence-aware & overlap chunkers
data/              â†’ FIA PDFs (2018â€“2026)
embeddings/        â†’ OpenAI embedding wrapper
index/             â†’ ingestion, metadata, Pinecone adapter
rag/               â†’ end-to-end RAG pipeline
rerank/            â†’ cross-encoder reranker
guardrails/        â†’ input / context / output guards
eval/              â†’ latency + faithfulness evaluation
scripts/           â†’ runnable entry points
cache/             â†’ Redis helpers (keys, client)
config.py          â†’ centralized configuration
retriever_interface.py â†’ clean DB-agnostic retriever interface


Each component is intentionally decoupled so that:

vector DBs can be swapped

rerankers can be changed

evaluation is independent of retrieval logic

ğŸš€ Getting Started (Run Locally)
1ï¸âƒ£ Clone & install dependencies
git clone <your-repo-url>
cd FIA_PROD_RAG

python -m venv .venv
source .venv/bin/activate  # or .venv\Scripts\activate on Windows

pip install -r requirements.txt

2ï¸âƒ£ Set environment variables

Create a .env file:

OPENAI_API_KEY=your_openai_key

PINECONE_API_KEY=your_pinecone_key
PINECONE_INDEX=fia-regulations
PINECONE_NAMESPACE=fia_prod

CACHE_ENABLED=1
CACHE_EMBEDDINGS=1
CACHE_RETRIEVAL=1
REDIS_HOST=localhost
REDIS_PORT=6379

EMBEDDING_MODEL=text-embedding-3-small
GEN_MODEL=gpt-4.1-mini


Make sure Redis is running:

redis-server

3ï¸âƒ£ Index the documents
python -m scripts.build_index


This will:

load & clean PDFs

chunk documents

infer metadata

store text in SQLite

store vectors + metadata in Pinecone

4ï¸âƒ£ Ask a question
python -m scripts.test_rag


Example query:

â€œWhat does Article 12.3 say about parc fermÃ© in 2026?â€

Output includes:

answer

citations (document, page, article)

debug info (cache hits, planner mode)

5ï¸âƒ£ Run evaluation
python -m scripts.run_eval


Generates:

eval_report.json

latency stats (mean / p50 / p95)

cache hit rates

faithfulness score

ğŸ“Š Sample Performance (Local)

Latency (p50): ~2.3s

Latency (p95): ~6â€“7s

Embedding cache hit rate: ~1.0

Retrieval cache hit rate: ~1.0

Faithfulness score: ~0.8â€“0.9 (strict judge)

ğŸ§© Design Decisions & Trade-offs

Pinecone vs FAISS â†’ managed scaling, metadata filters

SQLite DocStore â†’ simple, fast text hydration

Cross-encoder rerank â†’ deterministic, cheaper than LLM rerank

Redis caching â†’ largest latency reduction lever

Evaluation first â†’ changes are measured, not guessed

ğŸ”® Future Extensions (Optional)

UI (Streamlit / React)

Auth & multi-tenant access

Streaming responses

Feedback-driven retrieval tuning

Production deployment (AWS/GCP)

ğŸ‘¤ About This Project

This project was built to reflect real AI Engineer work, not a demo:

system thinking

performance trade-offs

evaluation & iteration

production-style abstractions

It is suitable as a flagship portfolio project for AI / ML / Applied LLM Engineer roles.
